{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUQ1pGLPfIcC"
   },
   "source": [
    "# Libraries and Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZnsJf5EDTMS"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/sam2.git\n",
    "%cd sam2\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfbwwk-8ZVqB"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cysjm28ELV4A",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You're likely running Python from the parent directory of the sam2 repository (i.e. the directory where https://github.com/facebookresearch/sam2 is cloned into). This is not supported since the `sam2` Python package could be shadowed by the repository name (the repository is also named `sam2` and contains the Python package in `sam2/sam2`). Please run Python from another directory (e.g. from the repo dir rather than its parent dir, or from your home directory) after installing SAM 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocotools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mask \u001b[38;5;28;01mas\u001b[39;00m coco_mask\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_opset11\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hstack\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_sam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_sam2\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msam2_image_predictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n",
      "File \u001b[1;32m~\\Documents\\Coral Research\\Algorithm Workspace\\Code Files\\sam2\\sam2\\build_sam.py:25\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check if the user is running Python from the parent directory of the sam2 repo\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# (i.e. the directory where this repo is cloned into) -- this is not supported since\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# it could shadow the sam2 package and cause issues.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sam2\u001b[38;5;241m.\u001b[39m__path__[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msam2\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# If the user has \"sam2/sam2\" in their path, they are likey importing the repo itself\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# as \"sam2\" rather than importing the \"sam2\" python package (i.e. \"sam2/sam2\" directory).\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# This typically happens because the user is running Python from the parent directory\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# that contains the sam2 repo they cloned.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre likely running Python from the parent directory of the sam2 repository \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(i.e. the directory where https://github.com/facebookresearch/sam2 is cloned into). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is not supported since the `sam2` Python package could be shadowed by the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepository name (the repository is also named `sam2` and contains the Python package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min `sam2/sam2`). Please run Python from another directory (e.g. from the repo dir \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrather than its parent dir, or from your home directory) after installing SAM 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     35\u001b[0m HF_MODEL_ID_TO_FILENAMES \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/sam2-hiera-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/sam2/sam2_hiera_t.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     ),\n\u001b[0;32m     68\u001b[0m }\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam2\u001b[39m(\n\u001b[0;32m     72\u001b[0m     config_file,\n\u001b[0;32m     73\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     79\u001b[0m ):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You're likely running Python from the parent directory of the sam2 repository (i.e. the directory where https://github.com/facebookresearch/sam2 is cloned into). This is not supported since the `sam2` Python package could be shadowed by the repository name (the repository is also named `sam2` and contains the Python package in `sam2/sam2`). Please run Python from another directory (e.g. from the repo dir rather than its parent dir, or from your home directory) after installing SAM 2."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "from torch.onnx.symbolic_opset11 import hstack\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "soktN5v8a2PC"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = YOLO(\"C:/Users/richa_0/Documents/Coral Research/Machine Learning Models/V2 Developmental Models/2612-augmented.yolov11/results/50_epochs-/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWuhkA5bfOwz"
   },
   "source": [
    "# Dataset Manipulation & Extraction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W2ztzhsNLZVK"
   },
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/richa_0/Documents/Coral Research/Machine Learning Models/V2 Developmental Models/2526-augmented.sam2/\"\n",
    "train_dir = os.path.join(data_dir, \"train/\")\n",
    "test_dir = os.path.join(data_dir, \"test/\")\n",
    "valid_dir = os.path.join(data_dir, \"valid/\")\n",
    "\n",
    "def extract_dataset(directory):\n",
    "    image_arrays = {}\n",
    "    binary_masks = {}\n",
    "    bbox_coords = {}\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            json_file_path = os.path.join(directory, file_name)\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                coco_data = json.load(f)\n",
    "            annotations = coco_data['annotations']\n",
    "            image = coco_data['image']\n",
    "\n",
    "            image_id = image['image_id']\n",
    "            file_name = image['file_name']\n",
    "            image_path = os.path.join(directory, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_arrays[image_id] = image\n",
    "\n",
    "            for annotation in annotations:\n",
    "                image_id = annotation['id']\n",
    "                segmentation = annotation['segmentation']\n",
    "\n",
    "                results = model.predict(image, verbose=False)\n",
    "                for result in results:\n",
    "                    boxes = result.boxes\n",
    "                if boxes.xyxy.tolist():\n",
    "                    bbox = boxes.xyxy.tolist()[0]\n",
    "                else:\n",
    "                    bbox = [0, 1024, 0, 1024]\n",
    "                bbox_array = np.array(bbox)\n",
    "\n",
    "                bbox_coords[image_id] = (bbox_array)\n",
    "\n",
    "                binary_mask = coco_mask.decode(segmentation)\n",
    "                binary_masks[image_id] = binary_mask\n",
    "\n",
    "    return image_arrays, binary_masks, bbox_coords\n",
    "\n",
    "dataset_indices = {}\n",
    "dataset_index_ptr = {}\n",
    "\n",
    "def read_single(dataset_type, reset_epoch=False):\n",
    "    global dataset_indices, dataset_index_ptr\n",
    "\n",
    "    if (dataset_type == \"train\"):\n",
    "        image_arrays = train_image_arrays\n",
    "        binary_masks = train_binary_masks\n",
    "        bbox_coords = train_bbox_coords\n",
    "    elif (dataset_type == \"test\"):\n",
    "        image_arrays = test_image_arrays\n",
    "        binary_masks = test_binary_masks\n",
    "        bbox_coords = test_bbox_coords\n",
    "    elif (dataset_type == \"val\"):\n",
    "        image_arrays = valid_image_arrays\n",
    "        binary_masks = valid_binary_masks\n",
    "        bbox_coords = valid_bbox_coords\n",
    "\n",
    "    # Initialize indices if not already done\n",
    "    if dataset_type not in dataset_indices:\n",
    "        dataset_indices[dataset_type] = np.arange(len(image_arrays))\n",
    "        np.random.shuffle(dataset_indices[dataset_type])  # Shuffle indices\n",
    "        dataset_index_ptr[dataset_type] = 0  # Start at the beginning\n",
    "\n",
    "    # Reset and reshuffle if epoch is flagged to reset\n",
    "    if reset_epoch or dataset_index_ptr[dataset_type] >= len(dataset_indices[dataset_type]):\n",
    "        dataset_indices[dataset_type] = np.arange(len(image_arrays))\n",
    "        np.random.shuffle(dataset_indices[dataset_type])\n",
    "        dataset_index_ptr[dataset_type] = 0\n",
    "\n",
    "    # Fetch the next index and increment pointer\n",
    "    entry = dataset_indices[dataset_type][dataset_index_ptr[dataset_type]]\n",
    "    dataset_index_ptr[dataset_type] += 1  # Increment pointer\n",
    "\n",
    "    Img = image_arrays[entry]\n",
    "    mask = binary_masks[entry]\n",
    "    bbox = bbox_coords[entry]\n",
    "\n",
    "    return Img, mask, bbox\n",
    "\n",
    "def read_batch(dataset_type, current_iteration, interval, batch_size=4):\n",
    "    limage = []\n",
    "    lmask = []\n",
    "    lbbox = []\n",
    "    for i in range(batch_size):\n",
    "            image,mask,bbox = read_single(dataset_type, reset_epoch=(current_iteration % interval == 0))\n",
    "            limage.append(image)\n",
    "            lmask.append(mask)\n",
    "            lbbox.append(bbox)\n",
    "\n",
    "    return limage, np.array(lmask), np.array(lbbox)\n",
    "\n",
    "def return_dataset_size(dataset_type):\n",
    "    if (dataset_type == \"train\"):\n",
    "        return len(train_image_arrays)\n",
    "    elif (dataset_type == \"test\"):\n",
    "        return len(test_image_arrays)\n",
    "    elif (dataset_type == \"val\"):\n",
    "        return len(valid_image_arrays)\n",
    "\n",
    "\n",
    "def get_itr_interval(dataset_type, epochs):\n",
    "  if (dataset_type == \"train\"):\n",
    "      return (len(train_image_arrays) // 4) * epochs, (len(train_image_arrays) // 4)\n",
    "  elif (dataset_type == \"test\"):\n",
    "      return (len(test_image_arrays) // 4) * epochs, (len(test_image_arrays) // 4)\n",
    "  elif (dataset_type == \"val\"):\n",
    "      return (len(valid_image_arrays) // 4) * epochs, (len(valid_image_arrays) // 4)\n",
    "\n",
    "train_image_arrays, train_binary_masks, train_bbox_coords = extract_dataset(train_dir)\n",
    "test_image_arrays, test_binary_masks, test_bbox_coords = extract_dataset(test_dir)\n",
    "valid_image_arrays, valid_binary_masks, valid_bbox_coords = extract_dataset(valid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX_eQ8CvfaBB"
   },
   "source": [
    "# SAM 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYjGjjOjDYex",
    "outputId": "e5d7a039-4ae0-420c-90c2-219f22db02bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-59fbd0ea0f70>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() # mixed precision\n",
      "<ipython-input-25-59fbd0ea0f70>:73: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # cast to mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Step: 0, Loss: 0.0196, IOU: 0.9235\n",
      "New best model saved at iteration 0 with IoU: 0.9235\n",
      "Validation - Step: 1326, Loss: 0.0068, IOU: 0.9674\n",
      "New best model saved at iteration 1326 with IoU: 0.9674\n",
      "Validation - Step: 2652, Loss: 0.0065, IOU: 0.9697\n",
      "New best model saved at iteration 2652 with IoU: 0.9697\n",
      "Validation - Step: 3978, Loss: 0.0064, IOU: 0.9698\n",
      "New best model saved at iteration 3978 with IoU: 0.9698\n",
      "Validation - Step: 5304, Loss: 0.0064, IOU: 0.9705\n",
      "New best model saved at iteration 5304 with IoU: 0.9705\n",
      "Validation - Step: 6630, Loss: 0.0064, IOU: 0.9710\n",
      "New best model saved at iteration 6630 with IoU: 0.9710\n",
      "Validation - Step: 7956, Loss: 0.0062, IOU: 0.9712\n",
      "New best model saved at iteration 7956 with IoU: 0.9712\n",
      "Validation - Step: 9282, Loss: 0.0062, IOU: 0.9711\n",
      "Validation - Step: 10608, Loss: 0.0063, IOU: 0.9712\n",
      "Validation - Step: 11934, Loss: 0.0063, IOU: 0.9718\n",
      "New best model saved at iteration 11934 with IoU: 0.9718\n",
      "Validation - Step: 13260, Loss: 0.0061, IOU: 0.9718\n",
      "New best model saved at iteration 13260 with IoU: 0.9718\n",
      "Validation - Step: 14586, Loss: 0.0063, IOU: 0.9716\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "sam2_checkpoint = \"/content/gdrive/MyDrive/Coral SAM 2 Tuner Folder/sam2.1_hiera_base_plus.pt\" # path to model weight\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_b+.yaml\" #  model config\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\") # load model\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# Set training parameters\n",
    "\n",
    "predictor.model.sam_mask_decoder.train(True) # enable training of mask decoder\n",
    "predictor.model.sam_prompt_encoder.train(True) # enable training of prompt encoder\n",
    "predictor.model.image_encoder.train(True) # enable training of image encoder: For this to work you need to scan the code for \"no_grad\" and remove them all\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.cuda.amp.GradScaler() # mixed precision\n",
    "\n",
    "iterations, VALIDATION_INTERVAL = get_itr_interval(\"train\", 30)\n",
    "\n",
    "def compute_metrics(predictor, dataset_type, batch_size):\n",
    "    \"\"\"Computes validation/test metrics for the given dataset type.\"\"\"\n",
    "    predictor.model.eval()\n",
    "    total_iou, total_loss = 0, 0\n",
    "    num_samples = 0\n",
    "    itr, end = get_itr_interval(dataset_type, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(return_dataset_size(dataset_type) // batch_size):\n",
    "            image, mask, input_bbox = read_batch(dataset_type, current_iteration=itr + 1, interval=end, batch_size = 4)\n",
    "            if mask.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            predictor.set_image_batch(image)\n",
    "            mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=input_bbox,\n",
    "                mask_logits=None,\n",
    "                normalize_coords=True\n",
    "            )\n",
    "            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "                points=None, boxes=unnorm_box, masks=None\n",
    "            )\n",
    "            high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "                image_embeddings=predictor._features[\"image_embed\"],\n",
    "                image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=True,\n",
    "                repeat_image=False,\n",
    "                high_res_features=high_res_features,\n",
    "            )\n",
    "            prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "            gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "            prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "            seg_loss = (\n",
    "                -gt_mask * torch.log(prd_mask + 1e-5) - (1 - gt_mask) * torch.log((1 - prd_mask) + 1e-5)\n",
    "            ).mean()\n",
    "            inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "            iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "\n",
    "            total_loss += seg_loss.item()\n",
    "            total_iou += iou.mean().item()\n",
    "            num_samples += 1\n",
    "\n",
    "    return total_loss / num_samples, total_iou / num_samples\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_val_iou = -float('inf')  # Initialize to negative infinity to always save the best model\n",
    "best_model_path = \"/content/gdrive/MyDrive/Coral SAM 2 Tuner Folder/best_SAM2.pth\"\n",
    "\n",
    "for itr in range(iterations):\n",
    "    with torch.cuda.amp.autocast():  # cast to mixed precision\n",
    "        # Load data batch\n",
    "        image, mask, input_bbox = read_batch(\"train\", current_iteration=itr + 1, interval=VALIDATION_INTERVAL, batch_size = 4)  # Update the function to provide bounding boxes\n",
    "        if mask.shape[0] == 0:\n",
    "            continue  # Ignore empty batches\n",
    "\n",
    "        predictor.set_image_batch(image)  # Apply SAM image encoder to the image\n",
    "\n",
    "        # Prompt encoding using bounding boxes\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=input_bbox,  # Use bounding boxes here\n",
    "            mask_logits=None,\n",
    "            normalize_coords=True\n",
    "        )\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=unnorm_box,  # Pass the bounding boxes to the encoder\n",
    "            masks=None\n",
    "        )\n",
    "\n",
    "        # Mask decoder\n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"],\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=False,\n",
    "            high_res_features=high_res_features,\n",
    "        )\n",
    "        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # Upscale the masks to the original image resolution\n",
    "\n",
    "        # Segmentation loss calculation\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])  # Turn logit map to probability map\n",
    "        seg_loss = (\n",
    "            -gt_mask * torch.log(prd_mask + 1e-5) - (1 - gt_mask) * torch.log((1 - prd_mask) + 1e-5)\n",
    "        ).mean()  # Cross entropy loss\n",
    "\n",
    "        # Score loss calculation (Intersection over Union - IOU)\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "        loss = seg_loss + score_loss * 0.05  # Mix losses\n",
    "\n",
    "        # Apply backpropagation\n",
    "        predictor.model.zero_grad()  # Empty gradient\n",
    "        scaler.scale(loss).backward()  # Backpropagate\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Mixed precision\n",
    "\n",
    "    # Validation step\n",
    "    if itr % VALIDATION_INTERVAL == 0:\n",
    "        val_loss, val_iou = compute_metrics(predictor, dataset_type=\"val\", batch_size=4)\n",
    "        print(f\"Validation - Step: {itr}, Loss: {val_loss:.4f}, IOU: {val_iou:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_iou > best_val_iou:  # Check if this is the best IoU so far\n",
    "            best_val_iou = val_iou\n",
    "            torch.save(predictor.model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved at iteration {itr} with IoU: {val_iou:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_loss, test_iou = compute_metrics(predictor, dataset_type=\"test\", batch_size=4)\n",
    "print(f\"Test Results - Loss: {test_loss:.4f}, IOU: {test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gp64WXsJoYx"
   },
   "outputs": [],
   "source": [
    "#add training loss and validation loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
